# -*- coding: utf-8 -*-
"""Copy of PlantPal

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AJFrVDnkVmZ2OK5_BC5In45gOBAI0rEW
"""

!pip install tensorflowjs==4.12.0 tensorflow==2.15 keras==2.15

!pip install jax==0.4.21 jaxlib==0.4.21

!pip install matplotlib-venn

!apt-get -qq install -y graphviz && pip install pydot
import pydot

import tensorflowjs as tfjs

import tensorflow as tf
import keras

print("TensorFlow version:", tf.__version__)
print("Keras version:", keras.__version__)

"""# Data Preprocessing"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

import pandas as pd
import numpy as np
import cv2
import os
import zipfile
import shutil
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from shutil import copyfile

from tensorflow.keras.layers import Rescaling
from tensorflow.keras.layers import Normalization

# TensorFlow and Keras for Model and Preprocessing
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.layers import Rescaling, Normalization, RandomFlip, RandomRotation
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, BatchNormalization, Dropout
from tensorflow.keras.initializers import GlorotNormal
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.models import Model
from tensorflow import keras
from tensorflow.keras.optimizers import Adam

# Other Preprocessing Tools
import shutil
from keras.models import load_model
from keras.preprocessing.image import load_img, img_to_array
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.applications.resnet import ResNet101
from tensorflow.keras.applications import ResNet50
from keras.applications.vgg16 import VGG16
from tensorflow.keras.applications import EfficientNetV2L

# Evaluation Libraries
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import plotly.express as px
import matplotlib.image as mpimg

import pathlib

import random
import warnings
warnings.filterwarnings('ignore')
print('completed')

!pip freeze > requirements.txt

from google.colab import files

# Download the generated requirements.txt
files.download('requirements.txt')

import kagglehub

# Download latest version
path = kagglehub.dataset_download("srilutfiyadwiyeni/plantpal-dataset")

print("Path to dataset files:", path)

path = path + '/PlantPal Dataset'
train_path = path + '/Train'

os.listdir(train_path)
os.listdir(path + '/Validation')
os.listdir(path + '/Test')

os.listdir(path)

# Load Data
# from google.colab import drive
# drive.mount('/content/drive')
# # Specify the path to the ZIP file in Google Drive
# train_path = '/content/drive/Shareddrives/Capstone Team_C242-PS112/ML/Plant Disease Dataset/PlantPal Dataset/Train'

# os.listdir('/content/drive/Shareddrives/Capstone Team_C242-PS112/ML')
# os.listdir('/content/drive/Shareddrives/Capstone Team_C242-PS112/ML/Plant Disease Dataset')
# os.listdir('/content/drive/Shareddrives/Capstone Team_C242-PS112/ML/Plant Disease Dataset/PlantPal Dataset/Train')

# List directories in the specified path
list_train_dir = os.listdir(train_path)
print(list_train_dir)
print("Number of categories in training set:", len(list_train_dir))

"""# Training Set"""

import os

# train_directory = '/content/drive/Shareddrives/Capstone Team_C242-PS112/ML/Plant Disease Dataset/PlantPal Dataset/Train'
train_directory = train_path
print(os.listdir(train_directory))

# show all diseases in dataset
diseases = os.listdir(train_directory)

# identify uniqe plant in dataset
plants = []
NumberOfDiseases = 0
for plant in diseases:
    if plant.split('___')[0] not in plants:
        plants.append(plant.split('___')[0])
print(f'number of different plants is :{len(plants)}')
print(plants)

for item in diseases:
    class_dir = os.path.join(train_directory, item)
    if os.path.exists(class_dir):
        print(item, " : ", len(os.listdir(class_dir)), "images")
    else:
        print(f"Directory not found: {class_dir}")

# Show example train images
plt.figure(figsize=(25, 4 * ((len(list_train_dir) + 4) // 5)))

for row in range((len(list_train_dir) + 4) // 5):
    for col in range(5):
        idx = row * 5 + col
        if idx >= len(list_train_dir):
            break

        plt.subplot((len(list_train_dir) + 4) // 5, 5, idx + 1)

        class_dir = os.path.join(train_directory, list_train_dir[idx])
        img_path = os.path.join(class_dir, os.listdir(class_dir)[0])
        img = mpimg.imread(img_path)

        plt.imshow(img)
        plt.axis('off')
        plt.title(list_train_dir[idx], fontsize=10, wrap=True)

plt.tight_layout()
plt.show()

"""  # Validation Set"""

# Define val image directory
# val_directory = '/content/drive/Shareddrives/Capstone Team_C242-PS112/ML/Plant Disease Dataset/PlantPal Dataset/Validation'
val_directory = path + '/Validation'

# Define path
# val_path = '/content/drive/Shareddrives/Capstone Team_C242-PS112/ML/Plant Disease Dataset/PlantPal Dataset/Validation'
# os.listdir('/content/drive/Shareddrives/Capstone Team_C242-PS112/ML/Plant Disease Dataset/PlantPal Dataset/Validation')

val_path = val_directory
os.listdir(val_path)

# List directories in the specified path
list_val_dir = os.listdir(val_path)
print(list_val_dir)
print("Number of categories in validation set:", len(list_val_dir))

# show all diseases in dataset
diseases_val = os.listdir(val_directory)
print(diseases)

# identify uniqe plant in dataset
plants = []
NumberOfDiseases = 0
for plant in diseases:
    if plant.split('___')[0] not in plants:
        plants.append(plant.split('___')[0])
print(f'number of different plants is :{len(plants)}')
print(plants)

# Iterate over the classes and print the number of images in each class folder
for item in list_val_dir:
    class_dir = os.path.join(val_directory, item)
    print(item, " : ", len(os.listdir(class_dir)), "images")

# Show example train images
plt.figure(figsize=(25, 4 * ((len(list_val_dir) + 4) // 5)))

for row in range((len(list_val_dir) + 4) // 5):
    for col in range(5):
        idx = row * 5 + col
        if idx >= len(list_val_dir):
            break

        plt.subplot((len(list_val_dir) + 4) // 5, 5, idx + 1)

        class_dir = os.path.join(val_directory, list_val_dir[idx])
        img_path = os.path.join(class_dir, os.listdir(class_dir)[0])
        img = mpimg.imread(img_path)

        plt.imshow(img)
        plt.axis('off')
        plt.title(list_val_dir[idx], fontsize=10, wrap=True)

plt.tight_layout()
plt.show()

"""# Exploratory Data Analysis (EDA)"""

import matplotlib.pyplot as plt
import os
import seaborn as sns

class_names = os.listdir(train_directory)
class_counts = {}

for class_name in class_names:
    class_counts[class_name] = len(os.listdir(os.path.join(train_directory, class_name)))

# Visualisasi distribusi kelas dengan palet warna yang diubah
plt.figure(figsize=(12, 8))

# Menggunakan palet warna Set3 dari seaborn
colors = sns.color_palette("Set3", len(class_counts))

plt.bar(class_counts.keys(), class_counts.values(), color=colors)
plt.xticks(rotation=90)
plt.title("Class Distribution in ML Training")
plt.xlabel("Class")
plt.ylabel("Number of images")
plt.show()

# Define val image directory
# val_directory = '/content/drive/Shareddrives/Capstone Team_C242-PS112/ML/Plant Disease Dataset/PlantPal Dataset/Validation'

# Check class distribution in val_directory
val_class_names = os.listdir(val_directory)
val_class_counts = {}

for val_class_name in val_class_names:
    val_class_counts[val_class_name] = len(os.listdir(os.path.join(val_directory, val_class_name)))

# Visualize class distribution for val_directory
plt.figure(figsize=(12, 8))

# Use the same Set3 color palette for consistency
val_colors = sns.color_palette("Set3", len(val_class_counts))

plt.bar(val_class_counts.keys(), val_class_counts.values(), color=val_colors)
plt.xticks(rotation=90)
plt.title("Class Distribution in ML Validation")
plt.xlabel("Class")
plt.ylabel("Number of images")
plt.show()

"""# Preparation for Modelling"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

def train_val_generators(TRAINING_DIR, VALIDATION_DIR):
    # Instantiate the ImageDataGenerator class for training
    train_datagen = ImageDataGenerator(rescale=1.0/255.,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest')

    # Flow training data from directory, resize images to (224, 224)
    train_generator = train_datagen.flow_from_directory(
        directory=TRAINING_DIR,
        batch_size=100,
        class_mode='categorical',
        target_size=(224, 224)  # Resize images to 224x224
    )

    # Instantiate the ImageDataGenerator class for validation
    validation_datagen = ImageDataGenerator(rescale=1.0/255.)

    # Flow validation data from directory, resize images to (224, 224)
    validation_generator = validation_datagen.flow_from_directory(
        directory=VALIDATION_DIR,
        batch_size=100,
        class_mode='categorical',
        shuffle=False,
        target_size=(224, 224)  # Resize images to 224x224
    )

    return train_generator, validation_generator

# Define paths to the training and validation directories
TRAINING_DIR =  train_path
VALIDATION_DIR =  path + '/Validation'

# Get the generators
train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)

# Extract labels
labels = []
for label in train_generator.class_indices:
    labels.append(label)

num_labels = len(labels)
print("Labels: {}".format(labels))

# Example to check the shape of one batch of images
x_batch, y_batch = next(train_generator)
print(f"Image batch shape: {x_batch.shape}")

def plot_accuracy(history):

    plt.plot(history.history['accuracy'],label='train accuracy')
    plt.plot(history.history['val_accuracy'],label='validation accuracy')
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(loc='best')
    plt.savefig('Accuracy_v1_InceptionV3')
    plt.show()

def plot_loss(history):

    plt.plot(history.history['loss'],label="train loss")
    plt.plot(history.history['val_loss'],label="validation loss")
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(loc='best')
    plt.savefig('Loss_v1_InceptionV3')
    plt.show()

def eval_CM(y_test, y_pred, labels):
    #Generate the confusion matrix
    cf_matrix3 = confusion_matrix(y_test, y_pred)

    ax3 = sns.heatmap(cf_matrix3, annot=True, fmt='g', cmap='Blues')

    ax3.set_title('Confusion Matrix with labels\n');
    ax3.set_xlabel('\nPredicted Values')
    ax3.set_ylabel('Actual Values ');

    ## Ticket labels - List must be in alphabetical order
    ax3.xaxis.set_ticklabels(labels, rotation=45)
    ax3.yaxis.set_ticklabels(labels, rotation=45)

    ## Display the visualization of the Confusion Matrix.
    return plt.show()

def lest_predict(model, uploaded):
    img = image.load_img(uploaded, target_size=(224,224))
    imgplot = plt.imshow(img)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    images = np.vstack([x])
    images /= 255 # because on train and test image is normalized, on image predict supposed to be too.
    classes = model.predict(images, batch_size=100) # the value is not always 1 and 0 because of probabilities
    predicted_class_indices=np.argmax(classes) # use to check prediction that have higher probabilities
    print(classes)
    print(labels[predicted_class_indices])

"""# CNN MODEL"""

from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import LeakyReLU

model = Sequential()

# Convolutional Layers with L2 Regularization and He Initialization
model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))  # Use LeakyReLU to avoid dead neurons
model.add(Conv2D(32, (3, 3), kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(64, (3, 3), kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))
model.add(Conv2D(64, (3, 3), kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(128, (3, 3), kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))
model.add(Conv2D(128, (3, 3), kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(2, 2))

# Replace Flatten with GlobalAveragePooling2D
model.add(GlobalAveragePooling2D())

# Dense Layers with Dropout and L2 Regularization
model.add(Dense(256, kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(Dropout(0.2))  # Lower dropout rate

model.add(Dense(128, kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(Dropout(0.2))

model.add(Dense(64, kernel_initializer='he_normal', kernel_regularizer=l2(0.0001)))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(Dropout(0.2))

model.add(Dense(31, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])

# Model summary
model.summary()

from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5, verbose=1)

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

# Ganti path berikut dengan path yang sesuai untuk sistem Anda
model_checkpoint_path = './saved_models/cnn_model.keras'  # Sesuaikan path ini

# Callbacks
model_checkpoint = ModelCheckpoint(model_checkpoint_path,
                                    monitor='val_accuracy',
                                    save_best_only=True,
                                    mode='max',
                                    verbose=1)

early_stopping = EarlyStopping(monitor='val_accuracy',
                               patience=3,
                               verbose=1,
                               mode='max',
                               restore_best_weights=True)

# Compile the model before training
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])

# Training the model
history = model.fit(train_generator,
                    validation_data=validation_generator,
                    epochs=25,
                    callbacks=[model_checkpoint, early_stopping])

# show learning curves
plot_accuracy(history)

plot_loss(history)

import tensorflow as tf
from tensorflow.keras import backend as K

# Precision
def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

# Recall
def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

# F1 Score
def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))

from sklearn.metrics import precision_score, recall_score, f1_score

# Prediksi data validasi
y_pred = model2.predict(validation_generator)
y_pred_classes = tf.argmax(y_pred, axis=1).numpy()  # Ambil label prediksi
y_true = validation_generator.classes  # Ambil label sebenarnya

# Hitung metrik
precision = precision_score(y_true, y_pred_classes, average='weighted')
recall = recall_score(y_true, y_pred_classes, average='weighted')
f1 = f1_score(y_true, y_pred_classes, average='weighted')

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

"""# Confusion Matrix"""

def eval_CM(y_test, y_pred, labels):
    import matplotlib.pyplot as plt
    from sklearn.metrics import confusion_matrix
    import seaborn as sns

    # Generate the confusion matrix
    cf_matrix3 = confusion_matrix(y_test, y_pred)

    # Set up the matplotlib figure with a larger size for better readability
    plt.figure(figsize=(15, 12))  # Adjust figure size as needed

    ax3 = sns.heatmap(cf_matrix3, annot=True, fmt='g', cmap='Blues', cbar=True)

    ax3.set_title('Confusion Matrix with Labels\n')
    ax3.set_xlabel('\nPredicted Values')
    ax3.set_ylabel('Actual Values ')

    # Set tick labels - Make sure labels correspond to the order used in the confusion matrix
    ax3.xaxis.set_ticklabels(labels, rotation=45, ha='right')
    ax3.yaxis.set_ticklabels(labels, rotation=45)

    # Display the visualization of the Confusion Matrix
    return plt.show()

y_test = validation_generator.classes
y_pred = model.predict(validation_generator)
y_pred = tf.argmax(y_pred, axis=1)

eval_CM(y_test, y_pred, labels)

"""# TEST CNN MODEL"""

import tensorflow as tf
import numpy as np
from PIL import Image
from IPython.display import display
from ipywidgets import FileUpload


# **Daftar kelas**
class_names = ['Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Apple___scab',
              'Corn___(maize)_Cercospora_leaf_spot Gray_leaf_spot', 'Corn___(maize)_Common_rust_',
              'Corn___(maize)_Northern_Leaf_Blight', 'Corn___(maize)_healthy', 'Grape___Black_rot',
              'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy',
              'Orange___Haunglongbing_(Citrus_greening)', 'Pepper___bell_Bacterial_spot', 'Pepper___bell_healthy',
              'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Squash___Powdery_mildew',
              'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight',
              'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot',
              'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot',
              'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']

# **Fungsi untuk memproses gambar**
def preprocess_image(image_path, target_size=(224, 224)):  # Sesuaikan target_size dengan model Anda
    image = Image.open(image_path).convert("RGB")
    image = image.resize(target_size)
    img_array = np.array(image) / 255.0  # Normalisasi
    img_array = np.expand_dims(img_array, axis=0)  # Menambahkan batch dimension
    return img_array

# **Fungsi untuk memprediksi kelas**
def predict_image(image_path):
    # Tampilkan gambar
    image = Image.open(image_path)
    display(image)

    # Preproses gambar
    img_array = preprocess_image(image_path)

    # Prediksi dengan model
    predictions = model.predict(img_array)[0]

    # Urutkan prediksi
    sorted_indices = np.argsort(predictions)[::-1]
    sorted_classes = [(class_names[i], predictions[i]) for i in sorted_indices]

    # Tampilkan hasil
    print("Prediksi:")
    for class_name, confidence in sorted_classes[:5]:  # Menampilkan 5 kelas teratas
        print(f"{class_name}: {confidence:.2%}")
    return sorted_classes

# **Unggah gambar untuk pengujian**
uploader = FileUpload(accept='image/*', multiple=False)

def on_upload_change(change):
    uploaded_file = next(iter(uploader.value.values()))
    file_name = uploaded_file['metadata']['name']
    with open(file_name, 'wb') as f:
        f.write(uploaded_file['content'])
    print(f"File '{file_name}' diunggah.")
    predict_image(file_name)

uploader.observe(on_upload_change, names='value')
display(uploader)

"""# SAVE CNN MODEL"""

model.save('cnn_model_plantpal.h5')

import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open('cnn_model_plantpal.tflite', 'wb') as f:
    f.write(tflite_model)

print("Model telah berhasil disimpan sebagai model.tflite")

model.save('plantpal_cnn_model.keras')

# Save the model in SavedModel format
model.save('saved_model/plantpal_cnn_model')

from google.colab import files
files.download('cnn_model_plantpal.h5')

# Simpan history ke file menggunakan pickle
import pickle

with open('history.pkl', 'wb') as file:
    pickle.dump(history.history, file)

print("History saved to history.pkl")

"""# Converter CNN Model"""

!tensorflowjs_converter --input_format=tf_saved_model /content/saved_model/plantpal_cnn_model /content/tfjs_cnn_model

!tensorflowjs_converter --input_format=keras cnn_model_plantpal.h5 /content/tfjs_cnn_model_from_h5

!zip -r models_cnn.zip /content/tfjs_cnn_model /content/saved_model /content/plantpal_cnn_model.h5 /content/plantpal_cnn_model.tflite /content/plantpal_cnn_model.keras /content/tfjs_cnn_model_from_h5 /content/history.pkl

import os
from google.colab import files

files.download('models_cnn.zip')

"""# MobileNet Model"""

from tensorflow.keras.applications import MobileNet

# Load model MobileNet dengan bobot pretrained dari ImageNet
model1 = MobileNet(weights='imagenet')

# Menampilkan arsitektur model
model1.summary()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.applications import MobileNet

# Load MobileNet tanpa output top (hanya bagian dasar model)
base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

base_model.trainable = True

# Hanya latih layer terakhir
for layer in base_model.layers[:-30]:
    layer.trainable = False

# Tambahkan layer baru
x = base_model.output
x = GlobalAveragePooling2D()(x)  # Pooling global
x = Dense(1024, activation='relu')(x)  # Fully connected layer
predictions = Dense(31, activation='softmax')(x)

# Membuat model baru
model2 = Model(inputs=base_model.input, outputs=predictions)

# Kompilasi model
model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Menampilkan arsitektur model
model2.summary()

from tensorflow.keras.utils import plot_model

plot_model(model2, show_shapes=True)

from tensorflow.keras.callbacks import ModelCheckpoint

# Tambahkan callback untuk menyimpan model terbaik berdasarkan akurasi validasi
checkpoint = ModelCheckpoint('best_model_MobileNet.h5', monitor='val_accuracy', save_best_only=True, verbose=1)

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)

history2 = model2.fit(
    train_generator,
    epochs=25,
    validation_data=validation_generator,
    callbacks=[checkpoint, early_stopping]
)

# show learning curves
plot_accuracy(history2)

plot_loss(history2)

# Convert predictions to class indices
y_pred_mn = model2.predict(validation_generator)
print(f"Predictions shape: {y_pred_mn.shape}")

# Ensure correct dimensions for argmax
if len(y_pred_mn.shape) == 2:  # Multi-class
    y_pred_mn = tf.argmax(y_pred_mn, axis=1)
elif len(y_pred_mn.shape) == 1:  # Binary
    y_pred_mn = tf.round(y_pred_mn)
else:
    raise ValueError("Unexpected shape of predictions: ", y_pred_mn.shape)

# Convert predictions to numpy if needed
y_pred_mn = y_pred_mn.numpy()

y_pred

# List to store predictions
y_pred_list = []

# Iterate through valid_data batch by batch
for images, labels in valid_data:
    # Predict for each batch of images
    y_pred_batch = model2.predict(images)

    # Append predictions to the list
    y_pred_list.append(y_pred_batch)

    # Optionally, break after processing a certain number of batches if needed
    # (e.g., stop after processing all images by checking the number of batches)
    if len(y_pred_list) * valid_data.batch_size >= len(valid_data.filenames):
        break

# Convert list of predictions to a single array
y_pred = np.concatenate(y_pred_list, axis=0)

# Now y_pred is a 1D array of predictions corresponding to the images in valid_data

# getthe real labels from valid_data
np.size(valid_data.classes)

valid_data.classes

# Get the true labels
y_true = valid_data.classes

# Get the mapping of class names to numerical labels
class_indices = valid_data.class_indices
print(class_indices)

labels = []
for label in train_data.class_indices:
  labels.append(label)
num_labels = len(labels)

print("Labels: {}".format(labels))

"""# Confusion Matrix"""

def eval_CM(y_test, y_pred_mn, labels):
    import matplotlib.pyplot as plt
    from sklearn.metrics import confusion_matrix
    import seaborn as sns

    # Generate the confusion matrix
    cf_matrix3 = confusion_matrix(y_test, y_pred_mn)

    # Set up the matplotlib figure with a larger size for better readability
    plt.figure(figsize=(15, 12))  # Adjust figure size as needed

    ax3 = sns.heatmap(cf_matrix3, annot=True, fmt='g', cmap='Blues', cbar=True)

    ax3.set_title('Confusion Matrix with Labels\n')
    ax3.set_xlabel('\nPredicted Values')
    ax3.set_ylabel('Actual Values ')

    # Set tick labels - Make sure labels correspond to the order used in the confusion matrix
    ax3.xaxis.set_ticklabels(labels, rotation=45, ha='right')
    ax3.yaxis.set_ticklabels(labels, rotation=45)

    # Display the visualization of the Confusion Matrix
    return plt.show()

eval_CM(y_test, y_pred_mn, labels)

"""# Test Model"""

import tensorflow as tf
import numpy as np
from PIL import Image
from IPython.display import display
from ipywidgets import FileUpload


# **Daftar kelas**
class_names = ['Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Apple___scab',
              'Corn___(maize)_Cercospora_leaf_spot Gray_leaf_spot', 'Corn___(maize)_Common_rust_',
              'Corn___(maize)_Northern_Leaf_Blight', 'Corn___(maize)_healthy', 'Grape___Black_rot',
              'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy',
              'Orange___Haunglongbing_(Citrus_greening)', 'Pepper___bell_Bacterial_spot', 'Pepper___bell_healthy',
              'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Squash___Powdery_mildew',
              'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight',
              'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot',
              'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot',
              'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']

# **Fungsi untuk memproses gambar**
def preprocess_image(image_path, target_size=(224, 224)):  # Sesuaikan target_size dengan model Anda
    image = Image.open(image_path).convert("RGB")
    image = image.resize(target_size)
    img_array = np.array(image) / 255.0  # Normalisasi
    img_array = np.expand_dims(img_array, axis=0)  # Menambahkan batch dimension
    return img_array

# **Fungsi untuk memprediksi kelas**
def predict_image(image_path):
    # Tampilkan gambar
    image = Image.open(image_path)
    display(image)

    # Preproses gambar
    img_array = preprocess_image(image_path)

    # Prediksi dengan model
    predictions = model2.predict(img_array)[0]

    # Urutkan prediksi
    sorted_indices = np.argsort(predictions)[::-1]
    sorted_classes = [(class_names[i], predictions[i]) for i in sorted_indices]

    # Tampilkan hasil
    print("Prediksi:")
    for class_name, confidence in sorted_classes[:5]:  # Menampilkan 5 kelas teratas
        print(f"{class_name}: {confidence:.2%}")
    return sorted_classes

# **Unggah gambar untuk pengujian**
uploader = FileUpload(accept='image/*', multiple=False)

def on_upload_change(change):
    uploaded_file = next(iter(uploader.value.values()))
    file_name = uploaded_file['metadata']['name']
    with open(file_name, 'wb') as f:
        f.write(uploaded_file['content'])
    print(f"File '{file_name}' diunggah.")
    predict_image(file_name)

uploader.observe(on_upload_change, names='value')
display(uploader)

"""# Save Model"""

# Simpan history2 ke file menggunakan pickle
import pickle

with open('history2.pkl', 'wb') as file:
    pickle.dump(history2.history, file)

print("History saved to history2.pkl")

# Konversi model Keras ke format TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model2)
tflite_model = converter.convert()

# Simpan sebagai file .tflite
with open("plantpal_model.tflite", "wb") as tflite_file:
    tflite_file.write(tflite_model)

model2.save('plantpal_model.keras')

# Save the model in SavedModel format
model2.save('saved_model/plantpal_model')

# Menyimpan model sebagai file.h5
model2.save('plantpal_model.h5')

# Mendownload model jika menggunakan Google Colab
files.download('plantpal_model.h5')

"""### Check"""

!ls -R /root/.cache/

print(os.getcwd())

# new_model = keras.models.load_model('plantpal_model.keras')

# Load the model from SavedModel format
# new_model = keras.models.load_model('saved_model/plantpal_model')

"""### Konversi"""

!tensorflowjs_converter --input_format=tf_saved_model /content/saved_model/plantpal_model /content/tfjs_model

!tensorflowjs_converter --input_format=keras plantpal_model.h5 /content/tfjs_model_from_h5

!zip -r models.zip /content/tfjs_model /content/saved_model /content/plantpal_model.h5 /content/plantpal_model.tflite /content/plantpal_model.keras /content/tfjs_model_from_h5 /content/history2.pkl

import os
from google.colab import files

files.download('models_plantpal.zip')

